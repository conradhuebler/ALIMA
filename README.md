# ALIMA

Ein leistungsstarkes Werkzeug zur automatisierten Schlagwortgenerierung und Klassifikation mit KI-Unterst√ºtzung, entwickelt an der Universit√§tsbibliothek "Georgius Agricola" der TU Bergakademie Freiberg.

## √úberblick

ALIMA ist eine Python-basierte Desktop-Anwendung, die fortschrittliche KI-Technologien mit bibliothekarischen Informationssystemen verbindet. Die Anwendung unterst√ºtzt bei der Generierung von pr√§zisen, GND-konformen Schlagw√∂rtern und der Zuweisung von DK/RVK-Klassifikationen.

## Hauptfunktionen

*   **Zentraler Pipeline-Workflow:** Eine einheitliche, mehrstufige Pipeline (Input ‚Üí Initialisierung ‚Üí GND-Suche ‚Üí Verifikation ‚Üí DK-Klassifikation) bildet den Kern der Analyse.
*   **Flexible Dateneingabe:** Unterst√ºtzung f√ºr Texteingabe, DOI/URL-Aufl√∂sung sowie die automatische Texterkennung (OCR) aus PDF- und Bilddateien.
*   **Multi-Provider-Unterst√ºtzung:** Kompatibel mit verschiedenen LLM-Anbietern (Ollama, Gemini, OpenAI-kompatible APIs).
*   **Intelligente Provider-Auswahl:** Ein "Smart Mode" w√§hlt automatisch den besten Provider und das beste Modell basierend auf der Aufgabe und benutzerdefinierten Pr√§ferenzen.
*   **Stapelverarbeitung:** M√∂glichkeit zur automatisierten Analyse einer gro√üen Anzahl von Dokumenten √ºber die Kommandozeile.
*   **Interaktive GUI:** Eine auf PyQt6 basierende Oberfl√§che zur Steuerung der Pipeline, Konfiguration und √úberpr√ºfung der Ergebnisse.

## Installation

### Voraussetzungen

*   Python 3.8 oder h√∂her
*   PyQt6

### Installationsschritte

1.  **Repository klonen:**
    ```bash
    git clone https://github.com/conradhuebler/ALIMA.git
    cd ALIMA
    ```

2.  **Virtuelle Umgebung erstellen und aktivieren:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # Unter Windows: venv\Scripts\activate
    ```

3.  **Abh√§ngigkeiten installieren:**
    ```bash
    pip install -r requirements.txt
    ```

## Konfiguration

Die Konfiguration von ALIMA erfolgt √ºber die Datei `config.json` im `~/.config/alima/`-Verzeichnis (Linux/macOS) oder `%APPDATA%\ALIMA\` (Windows). Die Anwendung bietet einen Einstellungsdialog, um diese Datei komfortabel zu verwalten.

Die Konfiguration der LLM-Provider ist im `unified_config`-Abschnitt zentralisiert und erm√∂glicht eine detaillierte Steuerung von Providern, Modellen und Aufgaben-Pr√§ferenzen.

### Struktur der `config.json`

Die `config.json` ist in mehrere Hauptbereiche unterteilt. Der wichtigste Abschnitt f√ºr die Steuerung der KI-Analyse ist `unified_config`.

```json
{
    "database_config": { ... },
    "catalog_config": { ... },
    "prompt_config": { ... },
    "system_config": { ... },
    "unified_config": {
        "providers": [
            {
                "name": "localhost",
                "provider_type": "ollama",
                "enabled": true,
                "api_key": "",
                "base_url": "http://localhost:11434",
                "preferred_model": "gemma:7b",
                "host": "http://localhost",
                "port": 11434,
                "use_ssl": false,
                "connection_type": "native_client"
            },
            {
                "name": "ChatAI",
                "provider_type": "openai_compatible",
                "enabled": true,
                "api_key": "DEIN_API_KEY",
                "base_url": "https://api.gwdg.de/ext/openai/v1",
                "preferred_model": "gpt-4o"
            }
        ],
        "task_preferences": {
            "keywords": {
                "task_type": "keywords",
                "model_priority": [
                    {
                        "provider_name": "localhost",
                        "model_name": "cogito:32b"
                    },
                    {
                        "provider_name": "gemini",
                        "model_name": "gemini-1.5-pro"
                    }
                ]
            },
            "initialisation": { ... }
        },
        "provider_priority": [
            "localhost",
            "gemini",
            "ChatAI"
        ]
    }
}
```

**Erl√§uterung des `unified_config`-Abschnitts:**

*   **`providers`**: Eine Liste aller konfigurierten LLM-Anbieter. Jeder Anbieter ist ein Objekt mit Typ (`ollama`, `openai_compatible`, `gemini`, etc.), URL, API-Schl√ºssel und einem optionalen `preferred_model`.
*   **`task_preferences`**: Hier k√∂nnen f√ºr spezifische Aufgaben (z.B. `keywords` f√ºr die Verifikation oder `initialisation` f√ºr die Erst-Analyse) feste Modell-Priorit√§ten definiert werden. ALIMA wird versuchen, die Modelle in der angegebenen Reihenfolge zu verwenden.
*   **`provider_priority`**: Eine globale Rangfolge der Provider, die verwendet wird, wenn f√ºr eine Aufgabe keine spezifische `task_preference` definiert ist.

## Verwendung

ALIMA kann √ºber die grafische Benutzeroberfl√§che (GUI) oder die Kommandozeile (CLI) genutzt werden.

### 1. Grafische Benutzeroberfl√§che (GUI)

Starten Sie die Anwendung mit:
```bash
python3 src/alima_gui.py
```
Der **"üöÄ Pipeline"-Tab** ist der zentrale Startpunkt f√ºr alle Analysen.

**Dateneingabe im Schritt "üì• Input & Datenquellen"**

Der erste Schritt bietet ein flexibles Eingabefeld mit mehreren Optionen:

*   **Text:** F√ºgen Sie einen Abstract oder beliebigen Text direkt in das Textfeld ein.
*   **DOI/URL:** Geben Sie eine DOI (z.B. `10.1007/...`) oder eine URL zu einem wissenschaftlichen Artikel ein. ALIMA versucht automatisch, den Inhalt aufzul√∂sen und den Volltext zu extrahieren.
*   **Datei laden (PDF & Bilder):**
    *   Klicken Sie auf den "Datei ausw√§hlen"-Button, um eine lokale Datei zu laden.
    *   **PDF-Dateien:** Das System extrahiert automatisch den Text aus der PDF. Bei gescannten Dokumenten oder PDFs ohne Textebene wird eine KI-basierte OCR (Texterkennung) versucht.
    *   **Bild-Dateien:** Bei Bildformaten (PNG, JPG etc.) wird automatisch eine KI-basierte OCR gestartet, um den im Bild enthaltenen Text zu extrahieren.

Nach der erfolgreichen Extraktion der Daten aus einer dieser Quellen k√∂nnen Sie die Analyse mit dem "üöÄ Auto-Pipeline"-Button starten.

### 2. Kommandozeilen-Nutzung (CLI)

Die ALIMA-CLI bietet zwei Hauptmodi f√ºr die Analyse: die `pipeline` f√ºr Einzelanalysen und `batch` f√ºr die Stapelverarbeitung.

#### 2.1. Einzelanalyse (`pipeline`-Befehl)

Der `pipeline`-Befehl f√ºhrt eine vollst√§ndige Analyse f√ºr eine einzelne Datenquelle durch.

**Eingabe-Optionen (einer erforderlich):**
*   `--input-text "..."`: Direkte Texteingabe.
*   `--doi "..."`: Eingabe einer DOI oder einer URL (wird automatisch aufgel√∂st).

**Beispiel:**
```bash
# F√ºhrt eine Standard-Analyse f√ºr einen Text durch
python3 src/alima_cli.py pipeline --input-text "Ein Text √ºber das Recycling von Lithium-Ionen-Batterien."

# F√ºhrt eine Analyse f√ºr eine DOI durch und speichert das Ergebnis
python3 src/alima_cli.py pipeline --doi "10.1007/s00442-021-04908-x" --output-json ergebnis.json
```

#### 2.2. Stapelverarbeitung (`batch`-Befehl)

Der `batch`-Befehl verarbeitet eine Liste von Datenquellen nacheinander.

**Argumente:**
*   `--batch-file <pfad>`: Pfad zu einer Textdatei, die die zu verarbeitenden Quellen enth√§lt (eine pro Zeile).
*   `--output-dir <ordner>`: Ordner, in dem die JSON-Ergebnisdateien gespeichert werden.

**Format der Batch-Datei:**
Jede Zeile muss das Format `TYP:WERT` haben. Unterst√ºtzte Typen sind `DOI`, `URL`, `PDF`, `IMG`, `TXT`.

```
# Beispiel batch_sources.txt
DOI:10.1007/s00442-021-04908-x
URL:https://www.tagesschau.de/wirtschaft/verbraucher/recycling-lithium-ionen-akkus-100.html
PDF:/home/user/docs/studie.pdf
IMG:/home/user/images/infografik.png
TXT:/home/user/docs/abstract.txt
```

**Beispiel-Aufruf:**
```bash
# F√ºhrt eine Batch-Analyse im Smart-Modus durch
python3 src/alima_cli.py batch --batch-file batch_sources.txt --output-dir ./results
```

## Lizenz
LGPL v3

## Mitwirkende
Conrad H√ºbler
Claude und Gemini AI

## Danksagung
Besten Dank an das Fachreferats- und IT-Team der Universit√§tsbibliothek. Besonderer Dank an Patrick Reichel f√ºr die effiziente Lobid-Abfrage.

## Kontakt
Conrad H√ºbler
