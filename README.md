# ALIMA

Ein leistungsstarkes Werkzeug zur automatisierten Schlagwortgenerierung und Klassifikation mit KI-Unterst√ºtzung  (Claude und Gemini) entwickelt an der Universit√§tsbibliothek "Georgius Agricola" der TU Bergakademie Freiberg.

## √úberblick

Der ALIMA ist eine Python-basierte Desktop-Anwendung, die fortschrittliche KI-Technologien mit bibliothekarischen Informationssystemen verbindet. Die Anwendung unterst√ºtzt bei der Generierung von pr√§zisen GND konformen Schlagw√∂rtern.
ALIMA wurde mit Hilfe von Claude Sonnet (3.5 und 4.0) und Gemini entwickelt. Ein erheblicher Teil der Dokumentation ist ebenfalls KI generiert, daher eine leichter √úbereuphrie √ºber noch nicht getestete Funktionen aber auch frisch implementierte ;-). Das wichtigste habe ich kommentiert.

## Funktionen

### 1. Unified Pipeline (Neu!)

ALIMA bietet jetzt einen einheitlichen Pipeline-Befehl f√ºr die vollst√§ndige Analyse von Input ‚Üí Initialisierung ‚Üí Suche ‚Üí Verifikation:

**Pipeline-Befehl**

```bash
python3 alima_cli.py pipeline [EINGABE-OPTIONEN] [LLM-OPTIONEN] [AUSGABE-OPTIONEN]
```

**Eingabe-Optionen (einer erforderlich):**

*   `--input-text "Text"`: Direkter Text-Input
*   `--doi "10.1007/123"`: DOI-Eingabe mit automatischer Aufl√∂sung (unterst√ºtzt Springer und CrossRef)
*   `--url "https://..."`: URL-Eingabe mit Web-Crawling (unterst√ºtzt Springer und generische Websites)

**LLM-Optionen:**

*   `--initial-model <model>`: Modell f√ºr Initialisierungsschritt (Standard: `cogito:14b`)
*   `--final-model <model>`: Modell f√ºr Verifikationsschritt (Standard: `cogito:32b`)
*   `--provider <provider>`: LLM-Anbieter (`ollama`, `gemini`, `openai`, `anthropic`) (Standard: `ollama`)
*   `--initial-task <task>`: Task f√ºr Initialisierung (`initialisation`, `keywords`, `rephrase`) (Standard: `initialisation`)
*   `--final-task <task>`: Task f√ºr Verifikation (`keywords`, `rephrase`, `keywords_chunked`) (Standard: `keywords`)
*   `--ollama-host <url>`: Ollama Host-URL (Standard: `http://localhost`)
*   `--ollama-port <port>`: Ollama Port (Standard: `11434`)

**Keyword Chunking (Neu!):**

*   `--keyword-chunking-threshold <anzahl>`: Aktiviert Chunking ab dieser Keyword-Anzahl (Standard: `500`)
*   `--chunking-task <task>`: Task f√ºr Chunk-Verarbeitung (`keywords_chunked`, `rephrase`) (Standard: `keywords_chunked`)

**Suchkonfiguration:**

*   `--suggesters <suggester1> <suggester2>`: GND-Suchprovider (`lobid`, `swb`) (Standard: `lobid swb`)

**Ausgabe-Optionen:**

*   `--output-json <dateipfad>`: Speichert vollst√§ndige Pipeline-Ergebnisse als JSON
*   `--resume-from <dateipfad>`: L√§dt Pipeline-Status und setzt fort

**Pipeline-Beispiele:**

```bash
# DOI-basierte Analyse mit Springer-Unterst√ºtzung
python3 alima_cli.py pipeline \
  --doi "10.1007/978-3-030-12345-6" \
  --initial-model "cogito:14b" \
  --final-model "cogito:32b" \
  --provider "ollama" \
  --ollama-host "http://139.20.140.163" \
  --ollama-port 11434 \
  --output-json "springer_analysis.json"

# URL-basierte Analyse mit Keyword Chunking
python3 alima_cli.py pipeline \
  --url "https://link.springer.com/book/10.1007/978-3-030-12345-6" \
  --keyword-chunking-threshold 300 \
  --chunking-task "keywords_chunked" \
  --output-json "url_analysis.json"

# Text-basierte Analyse mit benutzerdefinierten Tasks
python3 alima_cli.py pipeline \
  --input-text "Ihre Textanalyse hier..." \
  --initial-task "initialisation" \
  --final-task "rephrase" \
  --output-json "text_analysis.json"
```

### 2. Einzelschritte (Legacy)

ALIMA kann auch f√ºr einzelne Analyseschritte verwendet werden:

**Analyseaufgabe ausf√ºhren (`run` Befehl)**

```bash
python3 alima_cli.py run <task_name> --abstract "<ihr_abstract_text>" --model <model_name> [OPTIONEN]
```

**Gespeicherten Analyse-Status laden (`load-state` Befehl)**

```bash
python3 alima_cli.py load-state <eingabe_dateipfad>
```

## Installation

### Voraussetzungen

- Python 3.8 oder h√∂her
- PyQt6
- Weitere Abh√§ngigkeiten (siehe requirements.txt)

### Installationsschritte

**Repository klonen:**
```bash
git clone https://github.com/conradhuebler/ALIMA.git
cd ALIMA
```

**Virtuelle Umgebung erstellen und aktivieren:**
```bash
python3 -m venv venv
source venv/bin/activate  # Unter Windows: venv\Scripts\activate
```

**Basis-Abh√§ngigkeiten installieren:**
```bash
pip install -r requirements.txt
```

### üîß Optional: Erweiterte Web-Crawling-Funktionen

F√ºr erweiterte URL/DOI-Aufl√∂sung mit Springer-Unterst√ºtzung wird `crawl4ai` ben√∂tigt, auch wenn es erfolgreich via pip installiert ist, muss noch etwas dazu installiert werden. Das habe ich unter Windows nicht weiter getestet.

**Linux/macOS:**
```bash
pip install crawl4ai
```

**Windows (erweiterte Installation erforderlich):**
```bash
# Unter Windows kann crawl4ai zus√§tzliche Systemabh√§ngigkeiten erfordern
# Bei Installationsproblemen verwenden Sie WSL oder Docker

# Alternative: Basis-Installation ohne crawl4ai (DOI-Funktionalit√§t √ºber CrossRef API verf√ºgbar)
```

**Hinweise zu crawl4ai:**
- **Vollst√§ndige Funktionen**: Mit crawl4ai erhalten Sie erweiterte Springer-Unterst√ºtzung mit Inhaltsverzeichnis-Extraktion
- **Basis-Funktionalit√§t**: Ohne crawl4ai funktionieren DOI-Aufl√∂sungen √ºber CrossRef API weiterhin
- **Windows-Probleme**: Bei Installationsproblemen unter Windows verwenden Sie CrossRef-DOI-Aufl√∂sung oder WSL
- **Fallback-Verhalten**: ALIMA erkennt automatisch verf√ºgbare Funktionen und passt sich entsprechend an
### Konfigurationsdatei erstellen:

Erstelle eine Datei ~/.alima_config.json mit folgendem Inhalt:
```json
{
    "providers": ["openai", "chatai", "gemini", "anthropic"],
    "api_keys": {
        "openai": "YOUR_OPENAI_API_KEY",
        "chatai": "YOUR_CHATAI_API_KEY",
        "gemini": "YOUR_GEMINI_API_KEY",
        "anthropic": "YOUR_ANTHROPIC_API_KEY",
        "catalog_token": "YOUR_CATALOG_API_TOKEN",
        "catalog_search_url": "https://liberoserver/libero/LiberoWebServices.CatalogueSearcher.cls",
        "catalog_details" : "https://liberoserver/libero/LiberoWebServices.LibraryAPI.cls"
    }
}
```
F√ºr die Katalogsuche wird libero verwendet, zum Beispiel als liberoserver: ***libero.unibib.tu-edoras.rohan:443***. Die ollama-URL ist aktuell ***http://localhost:11434***, kann aber unter 
```bash
src/llm/llm_interface.py
```
angepasst werden.
### Anwendung starten:
```bash
python3 main.py
```

## Verwendung

Die ALIMA-Anwendung kann auf zwei Arten verwendet werden:

### 1. CLI-Nutzung

ALIMA kann √ºber die Kommandozeile f√ºr die Analyse von Texten und die Verwaltung von Analyseergebnissen verwendet werden. Die CLI unterst√ºtzt das Speichern und Laden von Aufgabenstatus als JSON-Dateien, was die Automatisierung und Wiederaufnahme von Aufgaben erm√∂glicht.

**1.1. Analyseaufgabe ausf√ºhren (`run` Befehl)**

Um eine Analyseaufgabe auszuf√ºhren und optional den Status in einer JSON-Datei zu speichern, verwenden Sie den `run`-Befehl:

```bash
python3 alima_cli.py run <task_name> --abstract "<ihr_abstract_text>" --model <model_name> [OPTIONEN]
```

**Argumente:**

*   `<task_name>`: Die auszuf√ºhrende Analyseaufgabe (z.B. `abstract`, `keywords`).
*   `--abstract "<ihr_abstract_text>"`: Der zu analysierende Abstract oder Text. In doppelten Anf√ºhrungszeichen einschlie√üen.
*   `--model <model_name>`: Das f√ºr die Analyse zu verwendende LLM-Modell (z.B. `cogito:8b`, `gemini-1.5-flash`).

**Optionen:**

*   `--keywords "<optionale_schlagworte>"`: Optionale Schlagworte, die in die Analyse einbezogen werden sollen. In doppelten Anf√ºhrungszeichen einschlie√üen.
*   `--provider <provider_name>`: Der zu verwendende LLM-Anbieter (z.B. `ollama`, `gemini`). Standard ist `ollama`.
*   `--ollama-host <host_url>`: Ollama Host-URL. Standard ist `http://localhost`.
*   `--ollama-port <port_number>`: Ollama Port. Standard ist `11434`.
*   `--use-chunking-abstract`: Chunking f√ºr den Abstract aktivieren (Flag).
*   `--abstract-chunk-size <gr√∂√üe>`: Chunk-Gr√∂√üe f√ºr den Abstract (Ganzzahl). Standard ist `100`.
*   `--use-chunking-keywords`: Chunking f√ºr Schlagworte aktivieren (Flag).
*   `--keyword-chunk-size <gr√∂√üe>`: Chunk-Gr√∂√üe f√ºr Schlagworte (Ganzzahl). Standard ist `500`.
*   `--output-json <dateipfad>`: Pfad zum Speichern der `TaskState`-JSON-Ausgabe. Wenn angegeben, wird der vollst√§ndige Status der Analyse in dieser Datei gespeichert.

**Beispiel:**

```bash
python3 alima_cli.py run abstract \
    --abstract "Dieses Buch behandelt die Kadmiumkontamination von B√∂den und Pflanzen..." \
    --model cogito:8b \
    --provider ollama \
    --ollama-host http://139.20.140.163 \
    --ollama-port 11434 \
    --use-chunking-abstract \
    --abstract-chunk-size 10 \
    --output-json meine_analyse.json
```

**1.2. Gespeicherten Analyse-Status laden (`load-state` Befehl)**

Um einen zuvor gespeicherten Analyse-Status aus einer JSON-Datei zu laden und anzuzeigen, verwenden Sie den `load-state`-Befehl:

```bash
python3 alima_cli.py load-state <eingabe_dateipfad>
```

**Argumente:**

*   `<eingabe_dateipfad>`: Pfad zur `TaskState`-JSON-Eingabedatei.

**Beispiel:**

```bash
python3 alima_cli.py load-state meine_analyse.json
```

Dies gibt den `full_text`, `matched_keywords` und `gnd_systematic` aus der geladenen JSON-Datei aus.

**1.3. Wichtige extrahierte und gespeicherte Informationen**

Wenn `--output-json` verwendet wird, werden die folgenden Informationen in der JSON-Datei gespeichert:

*   `abstract_data`: Der urspr√ºnglich bereitgestellte Abstract und die Schlagworte.
*   `analysis_result`: Enth√§lt die `full_text`-Antwort vom LLM, `matched_keywords` (aus der LLM-Ausgabe extrahiert) und `gnd_systematic` (aus der LLM-Ausgabe extrahiert).
*   `prompt_config`: Die Konfiguration des f√ºr die Analyse verwendeten Prompts.
*   `status`: Der Status der Aufgabe (z.B. `completed`, `failed`).
*   `task_name`: Der Name der ausgef√ºhrten Aufgabe.
*   `model_used`: Das verwendete LLM-Modell.
*   `provider_used`: Der verwendete LLM-Anbieter.
*   `use_chunking_abstract`, `abstract_chunk_size`, `use_chunking_keywords`, `keyword_chunk_size`: Verwendete Chunking-Einstellungen.

Dies erm√∂glicht eine reproduzierbare Analyse und die M√∂glichkeit, Ergebnisse programmgesteuert fortzusetzen oder weiterzuverarbeiten.

### 3. GUI-Nutzung

Die GUI-Anwendung bietet eine moderne, tab-basierte Oberfl√§che f√ºr die Analyse von Texten und die Verwaltung von Schlagworten.

**Starten:**
```bash
python3 main.py
```

**Hauptfunktionen:**

**Pipeline-Tab (Neu!):**
- **Auto-Pipeline**: Vollst√§ndige Analyse mit einem Klick
- **Vertikaler Workflow**: Chat-√§hnliche Darstellung der 5 Pipeline-Schritte
- **Live-Streaming**: Echtzeitanzeige der LLM-Token-Generierung
- **Visuelle Status-Indikatoren**: ‚ñ∑ (Wartend), ‚ñ∂ (L√§uft), ‚úì (Abgeschlossen), ‚úó (Fehler)
- **Integrierte Eingabe**: DOI, URL, PDF, Bild oder Text direkt im ersten Schritt
- **Keyword Chunking**: Automatische Aufteilung gro√üer Keyword-Sets (>500 Keywords)
- **JSON Export/Import**: Speichern und Fortsetzen von Pipeline-Zust√§nden - teilweise getestet

**Eingabem√∂glichkeiten:**
- **DOI-Eingabe**: Automatische Aufl√∂sung von Springer und CrossRef DOIs
- **URL-Eingabe**: Web-Crawling mit verbesserter Springer-Unterst√ºtzung - getestet -> es wird nur die ersten Seite des Inhaltsverzeichnis mit √ºbernommen
- **PDF-Analyse**: Texterkennung aus PDF-Dokumenten - geplant
- **Bildanalyse**: KI-basierte Bildbeschreibung und Schlagwortextraktion - geplant
- **Direkteingabe**: Manueller Text oder Clipboard-Import - getestet

**Weitere Tabs:**
- **Schlagwortsuche**: GND-Keyword-Suche und -verwaltung
- **Analyse-Review**: Detaillierte Ergebnisdarstellung und -validierung
- **CrossRef-Suche**: DOI-Metadaten-Lookup
- **UB-Suche**: Universit√§tsbibliothek-spezifische Suche

**Globale Statusleiste:**
- **Provider-Info**: Aktueller LLM-Provider und Modell
- **Cache-Statistiken**: Live-Anzeige der Datenbankgr√∂√üe und Eintr√§ge
- **Pipeline-Fortschritt**: Echtzeitanzeige des aktuellen Schritts


## üöÄ Aktuelle Pipeline-Funktionen (Version 1.0)

### ‚úÖ Vollst√§ndig implementiert und getestet

**Unified Pipeline Architecture:**
- **CLI & GUI Parit√§t**: Beide Interfaces verwenden identische Pipeline-Logik
- **5-Stufen-Workflow**: Input ‚Üí Initialisierung ‚Üí Suche ‚Üí Verifikation ‚Üí Klassifikation (letzter  Schritt l√§uft noch nicht)
- **Keyword Chunking**: Intelligente Aufteilung gro√üer Keyword-Sets mit konfigurierbarem Schwellenwert
- **Enhanced DOI/URL Support**: Springer-Crawling mit Inhaltsverzeichnis und CrossRef-API-Integration
- **Real-time Streaming**: Live-Token-Display w√§hrend LLM-Generierung
- **JSON Persistence**: Vollst√§ndiges Speichern/Fortsetzen von Pipeline-Zust√§nden

**LLM Provider Support:**
- **Ollama**: Lokale Modelle (cogito:14b, cogito:32b, magistral:latest, etc.)
- **Gemini**: Google AI Platform (gemini-1.5-flash, gemini-1.5-pro)
- **OpenAI**: GPT-3.5/4 Modelle - deaktiviert, aber ChatAI von GWDG funktioniert 
- **Anthropic**: Claude-Modelle - deaktiviert
- **Multi-Model Pipelines**: Verschiedene Modelle f√ºr verschiedene Schritte

**Input Sources:**
- **Text**: Direkteingabe, Clipboard-Import - getestet
- **DOI**: Automatische Aufl√∂sung (10.1007/... f√ºr Springer, andere √ºber CrossRef) - getestet
- **URL**: Web-Crawling (Springer-optimiert, generische Websites) - getestet
- **PDF**: Texterkennung (geplant)
- **Images**: KI-basierte Bildbeschreibung (geplant)

### üîß Technische Highlights

**Keyword Chunking System:**
- Automatische Erkennung bei >500 Keywords (konfigurierbar)
- Gleichm√§√üige Chunk-Verteilung (z.B. 1669 Keywords ‚Üí 4 Chunks [418, 417, 417, 417])
- Enhanced Keyword Recognition mit exakter String- und GND-ID-Erkennung
- Deduplizierung basierend auf Wort- oder GND-ID-Matching
- Finale Konsolidierung aller Chunk-Ergebnisse

**Unified DOI/URL Resolution:**
- **Springer Detection**: Automatische Erkennung von Springer-URLs und DOIs
- **Table of Contents Extraction**: Vollst√§ndige ToC-Extraktion f√ºr bessere Keyword-Analyse
- **Fallback Mechanisms**: CrossRef API als Backup f√ºr nicht-Springer DOIs
- **Error Handling**: Graceful Degradation bei crawl4ai-Problemen

## üìã Systemstatus

### ‚úÖ Produktionsreif
- **CLI Pipeline Command**: Vollst√§ndig funktional mit allen Features
- **GUI Pipeline Tab**: Auto-Pipeline-Button und Echtzeit-Feedback
- **Keyword Chunking**: Getestet mit gro√üen Keyword-Sets (>1500 Keywords)
- **DOI/URL Resolution**: Springer und CrossRef vollst√§ndig integriert

### üîÑ In Entwicklung
- **Batch Processing**: Mehrere Inputs parallel verarbeiten
- **Pipeline Templates**: Speichern/Laden verschiedener Workflow-Konfigurationen
- **Advanced Export**: Pipeline-Ergebnisse in verschiedenen Formaten (PDF, Excel)

### ‚ö†Ô∏è Bekannte Einschr√§nkungen
- **crawl4ai unter Windows**: Installationsprobleme aufgrund von Systemabh√§ngigkeiten - gibts auch unter Linux, habe es aber unter Windows nicht weiter testen k√∂nnen
- **Gro√üe Keyword-Sets**: Performance-Optimierung bei >2000 Keywords in Arbeit - Chunking sollte das Erledigen oder Gemini-Modelle
- **Gemini Seed Requirements**: Gemini-Modelle m√ºssen mit Seed=0 gestartet werden

### üß™ Getestete Modelle
- **Optimal**: cogito:32b f√ºr finale Keyword-Verifikation
- **Gut**: cogito:14b f√ºr Initialisierung, Gemma 3 27B f√ºr beide Schritte
- **Experimentell**: magistral:latest, gemini-1.5-flash f√ºr spezielle Anwendungen

# Lizenz
LGPL v3

# Mitwirkende
Conrad H√ºbler
Claude und Gemini AI
# Danksagung
Besten Dank an das Fachreferats- und IT-Team der Universit√§tsbibliothek. Besonderer Dank an Patrick Reichel f√ºr die effiziente Lobid-Abfrage.

# Kontakt
Conrad H√ºbler
